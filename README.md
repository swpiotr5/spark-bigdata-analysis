ğŸ“Š Citibike Big Data Analytics â€” Apache Spark Project
ğŸš´â€â™‚ï¸ Analysis of New York Citibike Trips Using Apache Spark
âš¡ High-performance ETL â€¢ Big Data Storage â€¢ Distributed Processing
ğŸ“Œ Project Overview

This project implements a full Big Data processing pipeline using Apache Spark, focusing on large-scale analysis of the Citibike NYC trip dataset.
The goal is to showcase the installation, configuration, storage, transformation, and exploration of massive datasets using modern distributed tools.

The dataset contains huge amount of records per year and is used to demonstrate:

- High-volume data ingestion

- Distributed processing with Spark

- Cleaning & harmonization of inconsistent schemas

- Storage in optimized formats (Parquet)

- Building a scalable analytical environment

ğŸš€ Key Features

âœ”ï¸ Full Big Data environment using Apache Spark
âœ”ï¸ ETL pipeline for ingesting, cleaning and merging rows
âœ”ï¸ Handling schema inconsistencies (e.g., duplicate 2017 columns)
âœ”ï¸ Storage in multiple formats: CSV, Parquet
âœ”ï¸ Efficient querying using Spark SQL
âœ”ï¸ Support for local Docker-based analytics environment
âœ”ï¸ Exploratory analysis & visualizations

ğŸ§± Tech Stack
| Category              | Tools                                    |
| --------------------- | ---------------------------------------- |
| **Processing Engine** | Apache Spark (PySpark)                   |
| **Environment**       | JupyterLab â€¢ Docker â€¢ Python 3.10        |
| **Storage Formats**   | CSV, Parquet                             |
| **Libraries**         | pandas, matplotlib, seaborn, pyspark.sql |
| **Data Source**       | Citibike NYC Trip Data                   |

ğŸ³ Project Architecture
```text
+---------------------------+
|        User (You)         |
|      Jupyter Notebook     |
+-------------+-------------+
              |
              v
+---------------------------+
|       Apache Spark        |
|  Distributed Processing   |
+-------------+-------------+
              |
              v
+---------------------------+
|   Data Lake / File Store  |
|    CSV  |  Parquet        |
+---------------------------+
```

ğŸ“‚ Directory Structure

```text
citibike-bigdata/
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml        # Spark + Jupyter environment
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ citibike_analysis.ipynb   # Main analysis notebook
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ citibike/                 # Raw yearly/monthly data
â”‚   â””â”€â”€ citibike_merged/          # Cleaned & merged yearly files
â”‚
â””â”€â”€ README.md
```

ğŸ”¥ What This Project Demonstrates
ğŸŸ¦ 1. Setting up Apache Spark locally (Docker)

Spark master + worker

JupyterLab with PySpark

Shared volume for easy development

ğŸŸ© 2. ETL: Loading millions of rows

Recursive scanning of year/month folders

Detecting & fixing inconsistent schemas

Handling malformed CSVs

Merging datasets into a unified yearly DataFrame

ğŸŸ¨ 3. Saving to high-performance formats

Efficient Parquet dumps

Automatic schema enforcement

Compression + optimized storage

ğŸŸ§ 4. Data exploration

ğŸ How to Run the Project

1ï¸âƒ£ Clone repo

git clone https://github.com/swpiotr5/spark-bigdata-analysis.git
cd citibike-bigdata

2ï¸âƒ£ Start Spark + Jupyter

docker compose up

3ï¸âƒ£ Open Jupyter

http://localhost:8888

4ï¸âƒ£ Run notebook

notebooks/citibike_analysis.ipynb


